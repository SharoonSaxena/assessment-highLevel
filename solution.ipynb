{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tomllib\n",
    "import pandas\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "from pandas import DataFrame, Series\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from time import time, ctime\n",
    "from joblib import Parallel, delayed\n",
    "from book_impact.preprocess import parallel_preprocess\n",
    "\n",
    "\n",
    "CONFIG = tomllib.load(open(\"config.toml\", \"rb\"))\n",
    "CONFIG_PREPROCESS = CONFIG[\"preprocess\"]\n",
    "CONFIG_MODEL = CONFIG[\"model\"]\n",
    "\n",
    "tracker: dict = deepcopy(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharoon/miniconda3/envs/highLevel/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "parallel processing: chunks-2; processes-2: 100%|██████████| 2/2 [00:00<00:00, 34.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(138724, 138724)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pandas.read_csv(\"books_task.csv\")\n",
    "tick = time()\n",
    "x, y = parallel_preprocess(data, CONFIG_PREPROCESS)\n",
    "tock = time()\n",
    "tracker[\"time_to_preprocess\"] = tock - tick\n",
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Title - Whispers of the Wicked Saints; description - Julia Thomas finds her life spinning out of control after the death of her husband, Richard. Julia turns to her minister for comfort when she finds herself falling for him with a passion that is forbidden by the church. Heath Sparks is a man of God who is busy taking care of his quadriplegic wife who was seriously injured in a sever car accident. In an innocent effort to reach out to a lonely member of his church, Heath finds himself as the man and not the minister as Heath and Julia surrender their bodies to each other and face the wrath of God. Julia finds herself in over her head as she faces a deadly disease, the loss of her home and whispers about her wicked affair. Julia leaves the states offering her body as a living sacrifice in hopes of finding a cure while her heart remains thousands of miles away hoping to one day reunite with the man who holds it hostage.Whispers of the Wicked Saints is a once in a lifetime romance that is breath taking, defying all the rules of romance and bending the laws of love.; authors - Veronica Haddon; publisher - iUniverse; publishedDate - 2005; categories - fiction; ',\n",
       " 666.4265418233589)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick inspection of the data\n",
    "idx = 3\n",
    "x[idx], y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader, Tokenizing, and model specific preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharoon/miniconda3/envs/highLevel/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "from book_impact.model import BertRegressor\n",
    "from transformers import AutoTokenizer\n",
    "import torch.optim as optim\n",
    "from torchmetrics import MeanAbsolutePercentageError\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['x', 'y'],\n",
       "        num_rows: 124851\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['x', 'y'],\n",
       "        num_rows: 13873\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = datasets.Dataset.from_dict({\n",
    "                                            \"x\": x,\n",
    "                                            \"y\": y,\n",
    "                                        })\n",
    "data_dict = train_data.train_test_split(test_size=CONFIG_PREPROCESS[\"test_split\"])\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 124851/124851 [00:14<00:00, 8668.59 examples/s] \n",
      "Map (num_proc=8): 100%|██████████| 13873/13873 [00:07<00:00, 1746.03 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['x', 'y', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 124851\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['x', 'y', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 13873\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_text(record):\n",
    "    return tokenizer(record['x'], truncation=True,\n",
    "            padding=\"max_length\", max_length=300)\n",
    "\n",
    "# tokenizing the text\n",
    "tick = time()\n",
    "data_dict[\"train\"] = data_dict[\"train\"].map(tokenize_text, batched=True, num_proc=CONFIG_PREPROCESS[\"workers\"])\n",
    "data_dict[\"test\"] = data_dict[\"test\"].map(tokenize_text, batched=True, num_proc=CONFIG_PREPROCESS[\"workers\"])\n",
    "data_dict[\"train\"] = data_dict[\"train\"].with_format(\"torch\")\n",
    "data_dict[\"test\"] = data_dict[\"test\"].with_format(\"torch\")\n",
    "tracker[\"tokenization_time\"] = time() - tick\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertRegressor(\n",
      "  (bert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear1): Linear(in_features=768, out_features=1024, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (linear2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (final_linear): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (out): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BertRegressor()\n",
    "tracker[\"model_architechture\"] = print(model)\n",
    "\n",
    "# loss function, Optimizer\n",
    "def loss_fn(output, target):\n",
    "    \"\"\"\n",
    "    Calculates Mean Absolute Percentage Error (MAPE) between model output and target values.\n",
    "\n",
    "    Args:\n",
    "        output (torch.Tensor): Model predictions tensor\n",
    "        target (torch.Tensor): Ground truth target values tensor\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: MAPE loss value computed on the device specified in CONFIG_MODEL\n",
    "    \"\"\"\n",
    "    mape = MeanAbsolutePercentageError().to(CONFIG_MODEL[\"device\"])\n",
    "    return mape(output.reshape(-1), target)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG_MODEL[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 : 100%|██████████| 976/976 [01:56<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mape: 3430.125244140625 | val_mape: 0.35510003566741943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 : 100%|██████████| 976/976 [01:57<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mape: 3470.838623046875 | val_mape: 0.3550058901309967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 : 100%|██████████| 976/976 [01:55<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mape: 3470.058349609375 | val_mape: 0.35490643978118896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 : 100%|██████████| 976/976 [01:54<00:00,  8.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mape: 3446.673828125 | val_mape: 0.35478267073631287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 : 100%|██████████| 976/976 [01:57<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mape: 3457.511474609375 | val_mape: 0.3548888564109802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 : 100%|██████████| 976/976 [01:56<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mape: 3440.36376953125 | val_mape: 0.35486936569213867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 : 100%|██████████| 976/976 [01:51<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mape: 3427.102294921875 | val_mape: 0.3547574579715729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 : 100%|██████████| 976/976 [01:49<00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mape: 3445.083984375 | val_mape: 0.35478177666664124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 : 100%|██████████| 976/976 [01:49<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mape: 3460.54833984375 | val_mape: 0.35483747720718384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 : 100%|██████████| 976/976 [01:49<00:00,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mape: 3458.347900390625 | val_mape: 0.3547438979148865\n"
     ]
    }
   ],
   "source": [
    "from book_impact.trainer import BertRegressorTraining\n",
    "\n",
    "trainer = BertRegressorTraining(data_dict=data_dict, optimizer=optimizer, loss_fn=loss_fn, model=model, config=CONFIG_MODEL)\n",
    "trainer.prepare_data_loader()\n",
    "trainer.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1, group2 = trainer.get_train_val_history()\n",
    "tracker[\"train_history_MAPE\"], tracker[\"val_history_MAPE\"] = [t.item() for t in group1], [t.item() for t in group2]\n",
    "tracker[\"remarks\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging experiment\n",
    "current_time = ctime()\n",
    "with open(f\"experiments/{current_time}.json\", \"w\") as f:\n",
    "    json.dump(tracker, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "highLevel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
