{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharoon/miniconda3/envs/highLevel/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import tomllib\n",
    "import pandas\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "from copy import deepcopy\n",
    "from itertools import chain\n",
    "from pandas import DataFrame, Series\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from time import time, ctime\n",
    "from joblib import Parallel, delayed\n",
    "from model import BertRegressor\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "CONFIG = tomllib.load(open(\"config.toml\", \"rb\"))\n",
    "CONFIG_PREPROCESS = CONFIG[\"preprocess\"]\n",
    "CONFIG_MODEL = CONFIG[\"model\"]\n",
    "\n",
    "tracker: dict = deepcopy(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processors\n",
    "def clean_authors(author_list: str) -> str:\n",
    "    return author_list.replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "\n",
    "def extract_year(date_str: str) -> str:\n",
    "    return date_str.split(\"-\")[0]\n",
    "\n",
    "def date_cleaner(string: str) -> str:\n",
    "    numbers =  ''.join(filter(str.isdigit, string))\n",
    "    if len(numbers) == 4:\n",
    "        return numbers\n",
    "    else:\n",
    "        return numpy.nan\n",
    "\n",
    "def category_cleaner(category_string: str) -> str:\n",
    "    return category_string.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").lower()\n",
    "\n",
    "def stringify_single(record: dict) -> str:\n",
    "    string = \"\"\n",
    "    for key, value in record.items():\n",
    "        string += f\"{key} - {value}; \"\n",
    "    return string\n",
    "\n",
    "def stringify_df(df: DataFrame) -> list:\n",
    "    records = df.to_dict(\"records\")[:5]\n",
    "    return [stringify_single(record) for record in records]\n",
    "\n",
    "\n",
    "def preprocess_pipe(df: DataFrame, config: dict) -> list| list:\n",
    "    #* Basic preprocessing\n",
    "    df = df.drop(config[\"drop_columns\"], axis=1)\n",
    "    df[\"description\"] = df[\"description\"].fillna(config[\"missing_string\"])\n",
    "    df[\"authors\"] = df[\"authors\"].fillna(config[\"missing_string\"])\n",
    "    df[\"publishedDate\"] = df[\"publishedDate\"].fillna(config[\"missing_string\"])\n",
    "    df[\"authors\"] = df[\"authors\"].map(clean_authors)\n",
    "    df[\"publishedDate\"] = df[\"publishedDate\"].map(extract_year).map(date_cleaner).dropna()\n",
    "    df[\"categories\"] = df[\"categories\"].map(category_cleaner)\n",
    "\n",
    "    #* model specific preprocessing\n",
    "    x_df =  df.drop(columns = [\"Impact\"], axis=1)\n",
    "    y: list = df[\"Impact\"].copy().to_list()\n",
    "    x: list = x_df.to_dict(\"records\")\n",
    "    string_x = stringify_df(x_df)\n",
    "    return string_x, y\n",
    "\n",
    "def parallel_preprocess(df: DataFrame, workers: int=CONFIG_PREPROCESS[\"workers\"], chunks: int= CONFIG_PREPROCESS[\"chunks\"], description: str = \"processing\", config: dict=CONFIG_PREPROCESS) -> (list, list):\n",
    "    chunked_df: list = numpy.array_split(df, chunks)\n",
    "\n",
    "    taskq =tqdm([delayed(preprocess_pipe)(chunk, config) for chunk in chunked_df], total=len(chunked_df), desc=description)\n",
    "    with Parallel(n_jobs=workers, verbose=0) as parallel:\n",
    "        chunk_xy = parallel(taskq) #[(x, y), (x, y)]\n",
    "\n",
    "    x, y = [], []\n",
    "    for chunk in chunk_xy:\n",
    "        chunk_x, chunk_y = chunk\n",
    "        x.extend(chunk_x)\n",
    "        y.extend(chunk_y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharoon/miniconda3/envs/highLevel/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "processing: 100%|██████████| 80/80 [00:00<00:00, 126.20it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pandas.read_csv(\"books_task.csv\")\n",
    "tick = time()\n",
    "x, y = parallel_preprocess(data)\n",
    "tock = time()\n",
    "tracker[\"time_to_preprocess\"] = tock - tick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Title - Whispers of the Wicked Saints; description - Julia Thomas finds her life spinning out of control after the death of her husband, Richard. Julia turns to her minister for comfort when she finds herself falling for him with a passion that is forbidden by the church. Heath Sparks is a man of God who is busy taking care of his quadriplegic wife who was seriously injured in a sever car accident. In an innocent effort to reach out to a lonely member of his church, Heath finds himself as the man and not the minister as Heath and Julia surrender their bodies to each other and face the wrath of God. Julia finds herself in over her head as she faces a deadly disease, the loss of her home and whispers about her wicked affair. Julia leaves the states offering her body as a living sacrifice in hopes of finding a cure while her heart remains thousands of miles away hoping to one day reunite with the man who holds it hostage.Whispers of the Wicked Saints is a once in a lifetime romance that is breath taking, defying all the rules of romance and bending the laws of love.; authors - Veronica Haddon; publisher - iUniverse; publishedDate - 2005; categories - fiction; ',\n",
       " 666.4265418233589)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick inspection of the data\n",
    "idx = 3\n",
    "x[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "x_inputs = tokenizer(x[:10], padding=True, truncation=True, return_tensors=\"pt\", max_length=CONFIG_MODEL[\"max_len\"], add_special_tokens=True).to(CONFIG_MODEL[\"device\"])\n",
    "y_inputs = torch.tensor(y[:10], dtype=torch.float32).to(CONFIG_MODEL[\"device\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertRegressor().to(CONFIG_MODEL[\"device\"])\n",
    "outs = model(ids=x_inputs[\"input_ids\"], mask=x_inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0448],\n",
       "         [-0.0602],\n",
       "         [-0.0464],\n",
       "         ...,\n",
       "         [-0.0396],\n",
       "         [-0.0376],\n",
       "         [-0.0470]],\n",
       "\n",
       "        [[-0.0379],\n",
       "         [-0.0468],\n",
       "         [-0.0321],\n",
       "         ...,\n",
       "         [-0.0327],\n",
       "         [-0.0373],\n",
       "         [-0.0214]],\n",
       "\n",
       "        [[-0.0363],\n",
       "         [-0.0599],\n",
       "         [-0.0612],\n",
       "         ...,\n",
       "         [-0.0483],\n",
       "         [-0.0386],\n",
       "         [-0.0403]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0625],\n",
       "         [-0.0759],\n",
       "         [-0.0539],\n",
       "         ...,\n",
       "         [-0.0649],\n",
       "         [-0.0607],\n",
       "         [-0.0634]],\n",
       "\n",
       "        [[-0.0558],\n",
       "         [-0.0521],\n",
       "         [-0.0492],\n",
       "         ...,\n",
       "         [-0.0472],\n",
       "         [-0.0374],\n",
       "         [-0.0464]],\n",
       "\n",
       "        [[-0.0422],\n",
       "         [-0.0645],\n",
       "         [-0.0516],\n",
       "         ...,\n",
       "         [-0.0374],\n",
       "         [-0.0449],\n",
       "         [-0.0320]]], device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging experiment\n",
    "current_time = ctime()\n",
    "with open(f\"experiments/{current_time}.json\", \"w\") as f:\n",
    "    json.dump(tracker, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "highLevel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
